
## Framework for XAI

Here are some useful references for Explainable AI (XAI):

1. [SHAP (SHapley Additive exPlanations)](https://github.com/slundberg/shap) - A game theoretic approach to explain the output of any machine learning model.

2. [LIME (Local Interpretable Model-agnostic Explanations)](https://github.com/marcotcr/lime) - Explains the predictions of any classifier in an interpretable and faithful manner. This is the [link to the paper](https://dl.acm.org/doi/10.1145/2939672.2939778).
<!-- 
3. [Interpretable Machine Learning](https://christophm.github.io/interpretable-ml-book/) - A guide for making black box models explainable by Christoph Molnar.

4. [Alibi](https://github.com/SeldonIO/alibi) - Algorithms for monitoring and explaining machine learning models.

5. [ELI5](https://eli5.readthedocs.io/en/latest/) - A library for debugging/inspecting machine learning classifiers and explaining their predictions.

6. [InterpretML](https://github.com/interpretml/interpret) - A toolkit to help understand models and enable responsible machine learning.

7. [AIX360](https://github.com/Trusted-AI/AIX360) - Algorithms and evaluation metrics for explainability of AI systems. -->

3. [What-If Tool](https://pair-code.github.io/what-if-tool/) - Visually probe the behavior of trained machine learning models, with minimal coding.

These resources provide a range of tools and methodologies for making AI models more interpretable and explainable.

## Concerns of models

1. [Concerns when interepretr Logistic Regression](https://academic.oup.com/esr/article-abstract/26/1/67/540767?redirectedFrom=fulltext) - This paper brings some concerns when using logistic regression for classification task. 

2. [Concerns when intereprete Decision Tree]()

3. [Concerns when interprete SVM]()

4. [Concerns when interprete Random Forest](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-8-25) - This paper introduces bias in random forest variable important measures.
